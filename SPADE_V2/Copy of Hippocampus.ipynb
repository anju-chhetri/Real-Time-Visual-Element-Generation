{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"0oSzY6H57H6T"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","!pip install -q pytorch-lightning\n","\n","!unzip /content/drive/MyDrive/grass/datasets/train.zip -d /content/train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b_gaxajh05SD"},"outputs":[],"source":["\n","import os\n","from glob import glob\n","from pathlib import Path\n","\n","import matplotlib.pyplot as plt\n","import pytorch_lightning as pl\n","import torch\n","from PIL import Image\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision import transforms\n","from torchvision.transforms.functional import center_crop\n","from torchvision.utils import make_grid, save_image\n","from tqdm.auto import tqdm\n","\n","import torch.nn.functional as F\n","from torch.nn import Module, Conv2d\n","from torch.nn.utils import spectral_norm\n","from torch.nn.functional import interpolate, relu\n","\n","import pandas as pd\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"OLCDJ_KeJDXA"},"outputs":[],"source":["config_defaults = {\n","    'BATCH_SIZE' : 8,\n","    'IN_CHANNEL' : 7,\n","    'OUT_CHANNEL' : 3,\n","    'LOAD_CHECKPOINT' : False,\n","    'PATH_CONTEXT' : '/content/train/',\n","    'PATH_DATA' : '/content/drive/MyDrive/grass/datasets/train.zip',\n","    'PATH_CHECKPOINT' : '/content/drive/MyDrive/grass/experiments/example-sea-creature-3/lightning_logs/version_21/checkpoints/epoch=8-step=4500.ckpt',\n","    'Lr_gen' : 0.0003,\n","    'Lr_disc' : 0.00001,\n","    'MAX_EPOCH' : 10,\n","    'SAVE_NTH_BATCH' : 50,\n","    'DATASET_SIZE' : 2000,\n","    'LAMBDA_RECON' : 100,\n","    'HALF_SIZE_LOSS_WEIGHT' : 1,\n","    'DISPLAY_LOSS_N': 100,\n","    'TRAIN_DISCRIMINATOR': 3,\n","    'RESULT_PATH': '/content/drive/MyDrive/grass/experiments/example-sea-creature-noise/',\n","    'INPUT_DIM': 50*30*128,\n","    'INITIAL_FILTER_SIZE': 128,\n","    }\n","\n","CONFIG = config_defaults"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AkrWA_hhc1Ga"},"outputs":[],"source":["class GrassDataset(Dataset):\n","  def __init__(self, path, num_items = -1):\n","    self.df = pd.read_csv(path + \"train.csv\")\n","    self.path = path\n","    self.length = num_items\n","    if num_items == -1:\n","      self.length = len(self.df)\n","    else:\n","      self.length = min(len(self.df),num_items)\n","      self.df = self.df.sample(n = self.length, replace=False)\n","    self.df = self.df.reset_index(drop=True)\n","    self.df.head()\n","\n","  def __len__(self):\n","    return self.length\n","\n","  def __getitem__(self, idx):\n","    inputImagePath = self.path + self.df['Input'][idx][2:]\n","       \n","    inputImage = Image.open(inputImagePath)\n","    inputImage = transforms.functional.to_tensor(inputImage)\n","\n","   \n","    depthImagePath = self.path + self.df['Depth'][idx][2:]\n","    depthImage = Image.open(depthImagePath)\n","    depthImage = transforms.functional.to_tensor(depthImage)\n","\n","    nImagePath = self.path + self.df['Normal'][idx][2:]\n","    nImage = Image.open(nImagePath)\n","    nImage = transforms.functional.to_tensor(nImage)\n","\n","\n","    realImagePath = self.path + self.df['Output'][idx][2:]\n","    realImage = Image.open(realImagePath)\n","    real = transforms.functional.to_tensor(realImage)\n","\n","    #                                       here\n","    condition = torch.cat((inputImage, depthImage[0:1, :, :], nImage), 0) # (3+1+3 = 7) x 480 x 800\n","\n","    return idx, real, condition\n","\n","class RAMGrassDataset(Dataset): \n","#This one loads the dataset onto the RAM, to speed up training speed, especially if you're running a large number of epochs\n","#Be very careful about how many items you want there to be here\n","  def __init__(self, path, num_items = -1):\n","    print(\"preparing RAM dataset, hopefully this doesn't take very long\")\n","    self.df = pd.read_csv(path + \"train.csv\")\n","    self.path = path\n","    self.length = num_items\n","    if num_items == -1:\n","      self.length = len(self.df)\n","    else:\n","      self.length = min(len(self.df),num_items)\n","      self.df = self.df.sample(n = self.length, replace=False)\n","    self.df = self.df.reset_index(drop=True)\n","\n","\n","    self.real = []\n","    self.condition = []\n","    print(\"prepared all variables, adding data to RAM\")\n","    for i in range(self.length):\n","      idx, real, condition = self.getitem(i)\n","      self.real.append(real)\n","      self.condition.append(condition)\n","    print(\"dataset added to RAM\")\n","\n","  def __len__(self):\n","    return self.length\n","\n","  def __getitem__(self, idx):\n","    return self.real[idx], self.condition[idx]\n","\n","  def getitem(self, idx):\n","    inputImagePath = self.path + self.df['Input'][idx][2:]\n","       \n","    inputImage = Image.open(inputImagePath)\n","    inputImage = transforms.functional.to_tensor(inputImage)\n","\n","    \n","    depthImagePath = self.path + self.df['Depth'][idx][2:]\n","    depthImage = Image.open(depthImagePath)\n","    depthImage = transforms.functional.to_tensor(depthImage)\n","\n","    nImagePath = self.path + self.df['Normal'][idx][2:]\n","    nImage = Image.open(nImagePath)\n","    nImage = transforms.functional.to_tensor(nImage)\n","\n","\n","    realImagePath = self.path + self.df['Output'][idx][2:]\n","    realImage = Image.open(realImagePath)\n","    real = transforms.functional.to_tensor(realImage)\n","\n","    #                                       here\n","    condition = torch.cat((inputImage, depthImage[0:1, :, :], nImage), 0) # (3+1+3 = 7) x 480 x 800\n","\n","    return idx, real, condition\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"b9lzDjZRdIlL"},"outputs":[],"source":["class DownSampleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel=4, strides=2, padding=1, activation=True, batchnorm=True):\n","\n","        super().__init__()\n","        self.activation = activation\n","        self.batchnorm = batchnorm\n","\n","        self.conv = nn.Conv2d(in_channels, out_channels, kernel, strides, padding)\n","\n","        if batchnorm:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","            #self.bn = nn.GroupNorm(1, out_channels)\n","\n","        if activation:\n","            self.act = nn.LeakyReLU(0.2)\n","\n","    def forward(self, x):\n","        x = self.conv(x)\n","        if self.batchnorm:\n","            x = self.bn(x)\n","        if self.activation:\n","            x = self.act(x)\n","        return x\n","\n","class UpSampleConv(nn.Module):\n","\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        kernel=4,\n","        strides=2,\n","        padding=1,\n","        activation=True,\n","        batchnorm=True,\n","        dropout=False\n","    ):\n","        super().__init__()\n","        self.activation = activation\n","        self.batchnorm = batchnorm\n","        self.dropout = dropout\n","\n","        self.deconv = nn.ConvTranspose2d(in_channels, out_channels, kernel, strides, padding)\n","\n","        if batchnorm:\n","            self.bn = nn.BatchNorm2d(out_channels)\n","\n","        if activation:\n","            self.act = nn.ReLU(True)\n","\n","        if dropout:\n","            self.drop = nn.Dropout2d(0.5)\n","\n","    def forward(self, x):\n","        x = self.deconv(x)\n","        if self.batchnorm:\n","            x = self.bn(x)\n","\n","        if self.dropout:\n","            x = self.drop(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"E3VnqoJTpPSB"},"outputs":[],"source":["# class ResnetBlock(nn.Module):\n","#     def __init__(self, dim, padding_type, norm_layer=nn.BatchNorm2d, activation=nn.ReLU(True), use_dropout=False):\n","#         super(ResnetBlock, self).__init__()\n","#         self.build_conv_block(dim, padding_type, norm_layer, activation, use_dropout)\n","\n","#     def build_conv_block(self, dim, padding_type, norm_layer, activation, use_dropout):\n","#         self.activation = activation;\n","#         self.norm_layer = norm_layer;\n","#         self.dropout = use_dropout;\n","\n","#         if(norm_layer):\n","#             self.nl = nn.BatchNorm2d(dim)\n","#             self.nl2 = nn.BatchNorm2d(dim)\n","\n","          \n","#         if activation:\n","#             self.act = nn.ReLU(True)\n","#             self.act2 = nn.ReLU(True)\n","          \n","#         if use_dropout:\n","#             self.dp = nn.Dropout(0.5)\n","\n","#         self.conv_1 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n","\n","\n","#         self.conv_2 = nn.Conv2d(dim, dim, kernel_size=3, padding=1)\n","\n","\n","#     def forward(self, x):\n","        \n","#         y = self.conv_1(x)\n","#         y = self.nl(y)\n","#         y = self.act(y)\n","\n","#         #y = self.dp(y) Regularisation\n","\n","#         y = self.conv_2(y)\n","#         y = self.nl2(y)\n","#         y = self.act2(y)\n","\n","#         return x + y"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"LDihXfJqdPps"},"outputs":[],"source":["# class RobGenerator(nn.Module):\n","#     def __init__(self, in_channels, out_channels):\n","#         super().__init__()\n","       \n","#         self.layers = [\n","#             DownSampleConv(in_channels, 64, batchnorm=False),  # bs x 64 x 240 x 400\n","#             DownSampleConv(64, 128),  # bs x 128 x 120 x 200\n","#             ResnetBlock(128, padding_type = 'reflect', use_dropout=True), # bs x 128 x 120 x 200\n","#             ResnetBlock(128, padding_type = 'reflect', use_dropout=True), # bs x 128 x 120 x 200\n","#             ResnetBlock(128, padding_type = 'reflect', use_dropout=True), # bs x 128 x 120 x 200\n","#             UpSampleConv(128, 128),  # bs x 128 x 240 x 400\n","#             ResnetBlock(128, padding_type = 'reflect', use_dropout=True), # bs x 128 x 240 x 400\n","#             ResnetBlock(128, padding_type = 'reflect', use_dropout=True), # bs x 128 x 240 x 400\n","#             nn.ConvTranspose2d(128, out_channels, kernel_size=4, stride=2, padding=1), # bs x 3 x 480 x 800\n","#         ]\n","\n","#         self.tanh = nn.Tanh()\n","#         self.layers = nn.ModuleList(self.layers)\n","\n","#     def forward(self, x):\n","#         for layer in self.layers:\n","#             x = layer(x)\n","#         return self.tanh(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5s-_hrHbxdxE"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"yk8cow6g6l1l"},"outputs":[],"source":["class SPADE(Module):\n","    def __init__(self, k):\n","        super().__init__()\n","        num_filters = 64\n","        kernel_size = 3\n","        self.conv = spectral_norm(Conv2d(CONFIG[\"IN_CHANNEL\"], num_filters, kernel_size=(kernel_size, kernel_size), padding=1)) #made changes on inputChannel\n","        self.conv_gamma = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1))\n","        self.conv_beta = spectral_norm(Conv2d(num_filters, k, kernel_size=(kernel_size, kernel_size), padding=1))\n","\n","    def forward(self, x, seg):\n","        #x = (b, 128, h, w), seg = (b, 4, h,w )\n","        # print('In SPADE')\n","        # seg_copy = self.convseg(seg) #seg = (b, 128, h,w)\n","        N, C, H, W = x.size()\n","        sum_channel = torch.sum(x.reshape(N, C, H*W), dim=-1)\n","\n","        mean = sum_channel / (N*H*W)\n","\n","        std = torch.sqrt((sum_channel**2 - mean**2) / (N*H*W))\n","        mean = torch.unsqueeze(torch.unsqueeze(mean, -1), -1)\n","        std = torch.unsqueeze(torch.unsqueeze(std, -1), -1)\n","        x = (x - mean) / std\n","\n","        # seg = interpolate(seg, size=(H,W), mode='nearest')\n","        seg_copy = relu(self.conv(seg))  #------------------------------->CPU and !CUDA\n","        # print(f'seg_copy: {seg_copy.shape}')\n","\n","        seg_gamma = self.conv_gamma(seg_copy)\n","        # print(f'seg_gamma: {seg_gamma.shape}')\n","\n","        seg_beta = self.conv_beta(seg_copy)\n","        # print(f'seg_beta: {seg_beta.shape}')\n","\n","        #torch.matmul performs matrix multiplication so for the given equal sized vectors need to make size of seg_gamma such that seg_gamma.size(3)=x.size(2)\n","        #so taking the difference between those two those indices concatinating the difference to x\n","        #x_copy = torch.cat((x, torch.zeros(x.size(0), x.size(1), seg_gamma.size(3)-x.size(2), x.size(3)).to(device)), dim = 2)\n","        # x = (torch.matmul(seg_gamma, x_copy) + seg_beta)\n","\n","        x = torch.mul(seg_gamma, x) + seg_beta\n","        # print(f'exit: {x.shape}')\n","        return x\n","\n","class SPADEResBlk(Module):\n","    def __init__(self, k, skip=False):\n","        super().__init__()\n","        kernel_size = 3\n","        self.skip = skip\n","        \n","        if self.skip:\n","            self.spade1 = SPADE(2*k)\n","            self.conv1 = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n","            self.spade_skip = SPADE(2*k)\n","            self.conv_skip = Conv2d(2*k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n","        else:\n","            self.spade1 = SPADE(k)\n","            self.conv1 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n","\n","        self.spade2 = SPADE(k)\n","        self.conv2 = Conv2d(k, k, kernel_size=(kernel_size, kernel_size), padding=1, bias=False)\n","    \n","    def forward(self, x, seg):\n","        # print(f'In SpadeResBlk')\n","        x_skip = x\n","        # print(f'before spade1: x {x.shape} seg: {seg.shape} ')\n","        x = relu(self.spade1(x, seg)) #b, 128, 30, 50\n","        # print(f'x: {x.shape}')\n","        # print(f'After spade1: {x.shape}')\n","        x = self.conv1(x) #b, 128, 30, 50\n","\n","        # print(f'After conv1: {x.shape}')\n","        x = relu(self.spade2(x, seg) ) #b, 128, 30,50  \n","        x = self.conv2(x) #b, 128, 30, 50\n","\n","        if self.skip:\n","            x_skip = relu(self.spade_skip(x_skip, seg))\n","            x_skip = self.conv_skip(x_skip)\n","        # print(f'After spade1: {(x_skip + x).shape}')\n","        return x_skip + x\n","\n","class SPADEGenerator(nn.Module):\n","    def __init__(self, inchannels, outchannels): #---------_Remember inchannels\n","        super().__init__()\n"," \n","        self.ln = nn.Linear(CONFIG['INITIAL_FILTER_SIZE'], CONFIG['INPUT_DIM'])\n","\n","        self.fc = nn.Conv2d(inchannels, 128, kernel_size = (3,3), padding = 1)\n","        self.spade_resblk1 = SPADEResBlk(128)\n","        self.spade_resblk2 = SPADEResBlk(128)\n","        self.spade_resblk3 = SPADEResBlk(64, skip=True)\n","        self.spade_resblk4 = SPADEResBlk(32, skip=True)\n","        self.conv = nn.utils.spectral_norm(nn.Conv2d(32, outchannels, kernel_size=(3,3), padding=1))\n","        self.convTrans128 = nn.ConvTranspose2d(128, 128, kernel_size = 2, stride = 2)\n","        self.convTrans64 = nn.ConvTranspose2d(64, 64, kernel_size = 2, stride = 2)\n","        self.convTrans32 = nn.ConvTranspose2d(32, 32, kernel_size = 2, stride = 2)\n","        self.noise = torch.normal(0, 1, (CONFIG['BATCH_SIZE'], CONFIG['INITIAL_FILTER_SIZE'])).to(device)\n","\n","\n","    \n","    def forward(self, seg):\n","  \n","        x = self.ln(self.noise)\n","        #print(f'After linear {x.shape}')\n","        x = x.view(CONFIG['BATCH_SIZE'], -1, 30, 50) #change 4 4 such that -1 = 128 If change this also change hidden layer size with x*y\n","        #print(f'After view {x.shape}')\n","        #x.shape = 8,4,15,25\n","        m4 = F.interpolate(seg, scale_factor = 0.5, mode = \"bicubic\") #b, 4, 240, 400\n","        m3 = F.interpolate(m4, scale_factor = 0.5, mode = \"bicubic\") #b, 4, 120, 200\n","        m2 = F.interpolate(m3, scale_factor = 0.5, mode = \"bicubic\") #b, 4, 60, 100\n","        m1 = F.interpolate(m2, scale_factor = 0.5, mode = \"bicubic\") #b, 4, 30, 50 \n","\n","        # print(f'm1: {m1.shape}')\n","        #x = self.fc(m2) #b, 128, 30, 50\n","        x = self.spade_resblk1(x, m1) #b, 128, 30, 50\n","        x= self.convTrans128(x)\n","        #x = F.interpolate(x, scale_factor = 2, mode='bicubic') #b, 128, 60, 100\n","    \n","        #print(f'interpolate1: {x.shape}')\n","\n","        x = self.spade_resblk2(x, m2) #b, 128, 60, 100\n","        #print(f'resblk2: {x.shape}')\n","        x= self.convTrans128(x)\n","        #x = F.interpolate(x, scale_factor = 2, mode='bicubic')#b, 128, 120, 200\n","        #print(f'interpolate 2: {x.shape}')\n","\n","        x = self.spade_resblk3(x, m3)  #b, 64, 120,200\n","        #print(f'resblk3: {x.shape}')\n","        x= self.convTrans64(x)\n","\n","       # x = F.interpolate(x,scale_factor = 2, mode='bicubic')#b, 64, 240, 400 \n","       # print(f'interpolate3: {x.shape}')\n","        \n","        x = self.spade_resblk4(x, m4)  #b, 32, 240, 400\n","        # print(f'resblk4: {x.shape}')\n","        x= self.convTrans32(x) #b,32,480,800\n","\n","       # x = F.interpolate(x, scale_factor = 2, mode='bicubic')#b, 32, 480, 800\n","        # print(f'interpolate 4: {x.shape}')\n","\n","        x = F.tanh(self.conv(x)) #b, 3, 480, 800\n","        # print(f'tanh: {x.shape}')\n","        \n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dzviNd6knUCP"},"outputs":[],"source":["# import torch\n","# l = torch.nn.Linear(Initial_filters, Height*Weight*Initial_Filters)\n","# x = torch.normal(0, 1, (batch_size, Initial_Filters))\n","# x = l(x)\n","# x = x.view(8, -1, 30, 50) #change 4 4 such that -1 = 128 If change this also change hidden layer size with x*y\n","# print(x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6hmHJp5UdPc6"},"outputs":[],"source":["class PatchGAN(nn.Module):\n","    def __init__(self, input_channels):\n","        super().__init__()\n","        self.d1 = DownSampleConv(input_channels, 32, batchnorm=False)\n","        self.d2 = DownSampleConv(32, 64)\n","        self.d3 = DownSampleConv(64, 64)\n","        self.d4 = DownSampleConv(64, 64)\n","        self.final = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, x, y):\n","        x = torch.cat([x, y], axis=1)\n","        x0 = self.d1(x)\n","        x1 = self.d2(x0)\n","        x2 = self.d3(x1)\n","        x3 = self.d4(x2)\n","        xn = self.final(x3)\n","        return xn\n","\n","class MultilevelPatchGAN(nn.Module):\n","    def __init__(self, input_channels):\n","        super().__init__()\n","        self.d1 = DownSampleConv(input_channels, 32, batchnorm=False)\n","        self.d2 = DownSampleConv(32, 64)\n","        self.d3 = DownSampleConv(64, 64)\n","        self.d4 = DownSampleConv(64, 64)\n","        self.final = nn.Conv2d(64, 1, kernel_size=1)\n","\n","        self.dd1 = DownSampleConv(input_channels, 32, batchnorm=False)\n","        self.dd2 = DownSampleConv(32, 64)\n","        self.dd3 = DownSampleConv(64, 64)\n","        self.dfinal = nn.Conv2d(64, 1, kernel_size=1)\n","\n","    def forward(self, x, y):\n","        x = torch.cat([x, y], axis=1)\n","        xx = torch.nn.functional.interpolate(x, scale_factor=0.5, mode=\"bicubic\")\n","\n","        x0 = self.d1(x)\n","        x1 = self.d2(x0)\n","        x2 = self.d3(x1)\n","        x3 = self.d4(x2)\n","        xn = self.final(x3)\n","\n","        xx0 = self.dd1(xx)\n","        xx1 = self.dd2(xx0)\n","        xx2 = self.dd3(xx1)\n","        xxn = self.dfinal(xx2)\n","        return xn, xxn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"3nrqMwgYdWaq"},"outputs":[],"source":["def _weights_init(m):\n","    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","    if isinstance(m, nn.BatchNorm2d):\n","        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n","        torch.nn.init.constant_(m.bias, 0)\n","        \n","def display_progress(cond, fake, real, figsize=(10,5)):\n","    cond = cond.detach().cpu().permute(1, 2, 0)\n","    fake = fake.detach().cpu().permute(1, 2, 0)\n","    real = real.detach().cpu().permute(1, 2, 0)\n","    \n","    fig, ax = plt.subplots(1, 3, figsize=figsize)\n","    #print(cond.shape)\n","    ax[0].imshow(cond[:,:,0:3])\n","    ax[2].imshow(fake)\n","    ax[1].imshow(real)\n","    plt.show()\n","\n","\n","def draw_result(lst_itr, lst_err1, lst_err2, label1, label2):\n","  plt.plot(lst_itr, lst_err1, '-b', label = label1)\n","  plt.plot(lst_itr, lst_err2, '-r', label = label2)\n","  plt.xlabel('iteration')\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dlrVMv_5daN5"},"outputs":[],"source":["class GAN(pl.LightningModule):\n","\n","    def __init__(self, in_channels, out_channels, learning_rate=0.0002, lambda_recon=200, next_save_idx = 0):\n","\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","\n","        self.gen = SPADEGenerator(in_channels, out_channels)\n","        self.patch_gan = PatchGAN(in_channels + out_channels)\n","\n","        # intializing weights\n","        self.gen = self.gen.apply(_weights_init)\n","        self.patch_gan = self.patch_gan.apply(_weights_init)\n","\n","        self.adversarial_criterion = nn.BCEWithLogitsLoss()\n","        self.recon_criterion = nn.L1Loss()\n","        self.trainidx = next_save_idx\n","\n","        self.lambda_recon = CONFIG[\"LAMBDA_RECON\"]\n","\n","        self.recon_lossl = []\n","        self.adverserial_lossl = []\n","        self.itrg = []\n","        self.itrd = []\n","        self.real_lossl = []\n","        self.fake_lossl = []\n","\n","    def _gen_step(self, real_images, conditioned_images):\n","        fake_images = self.gen(conditioned_images)\n","        disc_logits = self.patch_gan(fake_images, conditioned_images)\n","        adversarial_loss = self.adversarial_criterion(disc_logits, torch.ones_like(disc_logits))\n","\n","        recon_loss = self.recon_criterion(fake_images, real_images)\n","        self.adverserial_lossl.append(adversarial_loss.cpu().data.numpy())\n","        self.recon_lossl.append(recon_loss.cpu().data.numpy())\n","        self.itrg.append(len(self.adverserial_lossl))\n","        return self.lambda_recon *adversarial_loss +  recon_loss\n","        \n","    def _disc_step(self, real_images, conditioned_images):\n","        fake_images = self.gen(conditioned_images).detach()\n","\n","        fake_logits = self.patch_gan(fake_images, conditioned_images)\n","        real_logits = self.patch_gan(real_images, conditioned_images)\n","\n","        fake_loss = self.adversarial_criterion(fake_logits, torch.zeros_like(fake_logits))\n","        real_loss = self.adversarial_criterion(real_logits, torch.ones_like(real_logits))\n","        self.fake_lossl.append(fake_loss.cpu().data.numpy())\n","        self.real_lossl.append(real_loss.cpu().data.numpy())\n","        self.itrd.append(len(self.fake_lossl))\n","        return (real_loss + fake_loss) / 2\n","\n","    def configure_optimizers(self):\n","        gen_opt = torch.optim.Adam(self.gen.parameters(), lr=CONFIG['Lr_gen'])\n","        disc_opt = torch.optim.Adam(self.patch_gan.parameters(), lr=CONFIG['Lr_disc'])\n","        return disc_opt, gen_opt\n","\n","    def training_step(self, batch, batch_idx, optimizer_idx):\n","        idx, real, condition = batch\n","\n","        loss = None\n","\n","        if optimizer_idx == 1:\n","            loss = self._gen_step(real, condition)\n","\n","            self.log('Generator Loss', loss)\n","        \n","        elif(batch_idx%CONFIG['TRAIN_DISCRIMINATOR']==0 and optimizer_idx ==0 ):\n","\n","            loss = self._disc_step(real, condition)\n","\n","            self.log('PatchGAN Loss', loss)\n","        \n","\n","        if batch_idx% CONFIG['SAVE_NTH_BATCH']==0and optimizer_idx==1:\n","            print(f'batch size {batch_idx}')\n","            fake = self.gen(condition).detach()\n","            display_progress(condition[0], fake[0], real[0])\n","            for i in range(1):\n","              index = int(idx[i])+1\n","              path = CONFIG['RESULT_PATH'] + str(self.trainidx) + \"_\" + str(index) + \".png\"\n","              self.trainidx+=1\n","              save_image(fake[i], path)\n","              # draw_result(self.itrd, self.fake_lossl, self.real_lossl, label1 = 'Fake Loss', label2 = 'Real Loss') \n","              # draw_result(self.itrg, self.recon_lossl, self.adverserial_lossl, label1 = 'Recon Loss', label2 = 'Adverserial Loss') \n","\n","              # print(f'batch: {batch_idx}/{ CONFIG['DATASET_SIZE']/CONFIG['BATCH_SIZE']}')\n","\n","        if batch_idx%CONFIG['DISPLAY_LOSS_N'] ==0 and batch_idx != 0:\n","          draw_result(self.itrd, self.fake_lossl, self.real_lossl, label1 = 'Fake Loss', label2 = 'Real Loss') \n","          draw_result(self.itrg, self.recon_lossl, self.adverserial_lossl, label1 = 'Recon Loss', label2 = 'Adverserial Loss') \n","\n","\n","        return loss\n","\n","    def forward(self, x, y, z):\n","        x = torch.concat((x,y,z), axis=1)\n","        return self.gen(x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hndPFF_8x4HQ"},"outputs":[],"source":["#load dataset\n","dataset = GrassDataset(CONFIG['PATH_CONTEXT'], CONFIG['DATASET_SIZE'])\n","dataloader = DataLoader(dataset, batch_size=CONFIG['BATCH_SIZE'], shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":748},"executionInfo":{"elapsed":15637,"status":"error","timestamp":1678207685976,"user":{"displayName":"ANJU CHHETRI","userId":"06841725651527849321"},"user_tz":-345},"id":"_sTgvaZGdcFX","outputId":"74bc6e9f-30e1-4389-8047-0aff3b6d54ab"},"outputs":[{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-536935c7e7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PATH_CHECKPOINT'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IN_CHANNEL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'OUT_CHANNEL'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'MAX_EPOCH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_root_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RESULT_PATH'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-a29cf07bc023>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, learning_rate, lambda_recon, next_save_idx)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPADEGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_gan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-5b6355005355>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inchannels, outchannels)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvTrans64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvTrans32\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'BATCH_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'INITIAL_FILTER_SIZE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'torch.device' object is not callable"]}],"source":["xadversarial_loss = nn.BCEWithLogitsLoss()\n","reconstruction_loss = nn.L1Loss()\n","\n","if torch.cuda.is_available():\n","  device = torch.device('cuda')\n","else:\n","  device = torch.device('cpu')\n","\n","if CONFIG['LOAD_CHECKPOINT']:\n","  model = GAN.load_from_checkpoint(CONFIG['PATH_CHECKPOINT'])\n","else:\n","  model = GAN(CONFIG['IN_CHANNEL'], CONFIG['OUT_CHANNEL'])\n","\n","trainer = pl.Trainer(max_epochs=CONFIG['MAX_EPOCH'], gpus=1, default_root_dir = CONFIG['RESULT_PATH'])\n","trainer.fit(model, dataloader)"]},{"cell_type":"markdown","metadata":{"id":"xzPSohJemMHe"},"source":["### from 60*100 size\n","1. added recon factor in adverserial\n","\n","2. decreased learning rate of discriminator\n","3. Performed element wise multiplication\n","4. Changed the number of times the discriminator trains\n","\n","### in 30 * 50\n","1. Results are too smooth\n","\n","to-Do:\n","add update_learning_rate\n","\n","\n","\n","version: 21"]},{"cell_type":"markdown","metadata":{"id":"3mYuDrfIWr1_"},"source":["## After training:\n","1. the losses remain same after nth epoch\n","2. Recon loss : ~0.1\n","3. Real loss: ~0.67\n","4. fake loss: ~0.71\n","5. Adverserial: ~0.7"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XKsdwQxxWqg4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kKQzcIpjzuIC"},"outputs":[],"source":["import torch\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9dY02d9cg4ej"},"outputs":[],"source":["# import cv2\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# import torch\n","# from PIL import Image\n","# from torchvision.transforms import transforms\n","\n","# image = Image.open(\"/content/train/Input/0_input.jpg\")\n","# image.show()\n","# T = transforms.ToTensor()\n","# reI = T(image)\n","# reI = torch.unsqueeze(reI, dim = 0)\n","# print(reI.shape)\n","# rescaled1 = torch.nn.functional.interpolate(reI, scale_factor = 0.5, mode = \"bicubic\") # 200\n","# rescaled2 = torch.nn.functional.interpolate(rescaled1, scale_factor = 0.5, mode = \"bicubic\")#100\n","# rescaled3 = torch.nn.functional.interpolate(rescaled2, scale_factor = 0.5, mode = \"bicubic\")#50\n","\n","# rescaled1 = torch.squeeze(rescaled1)\n","# rescaled2 = torch.squeeze(rescaled2)\n","# rescaled3 = torch.squeeze(rescaled3)\n","\n","# I = transforms.ToPILImage()\n","# en1 = I(rescaled1)\n","# en2 = I(rescaled2)\n","# en3 = I(rescaled3)\n","# en1.show()\n","# en2.show()\n","# en3.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-HMfT686g6q3"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"5xpwD_Mr9oNO"},"source":["EXPERIMENT INFORMATION\n","\n","\n","Verson 13: Even after 9th epoch the results seems to be consistent. The model is not learning anything. It is learning about the lightning, structures and colors very well but it's not getting the textures right(or at all). Here we started training it from 60,100 size. \n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"132gXC4t1G1zYNJRy-G4gkqnQ6vSq5yW0","timestamp":1678025806740},{"file_id":"1YxwSWXnqa8ZqXKxumr-8tAALAeOPWZCd","timestamp":1678018306280},{"file_id":"1C2_SWrhPiSGAqw_m-aoKpxQojSbyLknd","timestamp":1678001506620},{"file_id":"1MIAf9roTtAYPWILFqcn1SkP0a52wkIaf","timestamp":1677940920502},{"file_id":"1MobP38rYsAUi5dCABeesbq5zI_5x1YBr","timestamp":1677838936587},{"file_id":"1CODWZ74KIXuFs5flmx0q9F8enhSDdRFd","timestamp":1677779502168},{"file_id":"1XBskyqqNQYWmInQHe5SIg0iFGTNZ8PKS","timestamp":1677265202584}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}